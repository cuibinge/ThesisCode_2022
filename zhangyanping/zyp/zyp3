import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision.models import resnet50

class KAM(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.q_conv = nn.Conv2d(channels, channels, 1)
        self.k_conv = nn.Conv2d(channels, channels, 1)
        self.v_conv = nn.Conv2d(channels, channels, 1)

    def forward(self, x):
        B, C, H, W = x.shape
        Q = F.softplus(self.q_conv(x)).view(B, C, -1)  # (B, C, N)
        K = F.softplus(self.k_conv(x)).view(B, C, -1)
        V = self.v_conv(x).view(B, C, -1)
        
        sim = torch.bmm(Q.transpose(1, 2), K)  # (B, N, N)
        alpha = F.softmax(sim, dim=-1)
        output = torch.bmm(V, alpha.transpose(1, 2))  # (B, C, N)
        return output.view(B, C, H, W)

class CAM(nn.Module):
    def __init__(self, channels, reduction=8):
        super().__init__()
        self.gap = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(channels, channels // reduction),
            nn.ReLU(),
            nn.Linear(channels // reduction, channels),
            nn.Sigmoid()
        )

    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.gap(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return x * y

class AKC(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.kam = KAM(channels)
        self.cam = CAM(channels)
        
    def forward(self, x):
        return self.kam(x) + self.cam(x)

class RAU(nn.Module):
    def __init__(self, in_channels=32):
        super().__init__()
        self.W_theta = nn.Sequential(
            nn.Conv2d(in_channels, 32, 1),
            nn.Sigmoid()
        )
        self.W_phi = nn.Sequential(
            nn.Conv2d(in_channels, 32, 1),
            nn.Sigmoid()
        )
        
    def forward(self, T1, T2):
        T1_prime = self.W_theta(T1)
        T2_prime = self.W_phi(T2)
        return T1_prime * T1 + T2_prime * T2 * (1 - T1_prime) + T1

class SBA(nn.Module):
    def __init__(self):
        super().__init__()
        self.rau1 = RAU()
        self.rau2 = RAU()
        self.conv3x3 = nn.Sequential(
            nn.Conv2d(64, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU()
        )
        
    def forward(self, Fs, Fb):
        out1 = self.rau1(Fs, Fb)
        out2 = self.rau2(Fb, Fs)
        return self.conv3x3(torch.cat([out1, out2], dim=1))

class ResNetBlock1(nn.Module):
    def __init__(self, in_channels):
        super().__init__()
        self.akc = AKC(in_channels)
        self.conv1 = nn.Conv2d(in_channels, 32, 1)
        self.sba = SBA()
        self.conv2 = nn.Conv2d(32, 32, 1)
        
    def forward(self, x):
        x = self.akc(x)
        x = self.conv1(x)
        x = self.sba(x, x)  # 示例特征传递，根据实际情况调整
        return self.conv2(x)

class ResNetBlock(nn.Module):
    def __init__(self, in_channels):
        super().__init__()
        self.akc = AKC(in_channels)
        self.conv1 = nn.Conv2d(in_channels, 32, 1)
        self.conv2 = nn.Conv2d(32, 32, 1)
        
    def forward(self, x):
        x = self.akc(x)
        x = self.conv1(x)
        return self.conv2(x)

class SEONet(nn.Module):
    def __init__(self):
        super().__init__()
        # Backbone Network
        backbone = resnet50(pretrained=True)
        self.backbone = nn.Sequential(
            backbone.conv1,
            backbone.bn1,
            backbone.relu,
            backbone.maxpool,
            backbone.layer1,  # layer1 output
            backbone.layer2,  # layer2 output
            backbone.layer3,  # layer3 output
            backbone.layer4   # layer4 output
        )
        
        # ResNet Blocks
        self.block1 = ResNetBlock1(256)
        self.block2 = ResNetBlock(512)
        self.block3 = ResNetBlock(1024)
        self.block4 = ResNetBlock(2048)
        
        # MLA模块（示例用3个AKC）
        self.mla = nn.Sequential(AKC(32), AKC(32), AKC(32))
        
        # SAB模块
        self.sab = SBA()
        self.final_conv = nn.Conv2d(32, 1, 1)
        
    def forward(self, x):
        # 提取Backbone特征
        x = self.backbone(x)
        
        # 处理不同层特征（示例，需根据实际输出调整）
        f1 = self.block1(x[3])  # 假设x[3]是layer4的输出
        f2 = self.block2(f1)
        f3 = self.block3(f2)
        f4 = self.block4(f3)
        
        # MLA处理
        mla_out = self.mla(f4)
        
        # SAB聚合（示例使用最后两个特征）
        sab_out = self.sab(mla_out, f1)
        
        return torch.sigmoid(self.final_conv(sab_out))

# 测试
if __name__ == "__main__":
    model = SEONet()
    input_tensor = torch.randn(1, 4, 128,128)
    output = model(input_tensor)
    print(output.shape)  # 预期输出: torch.Size([1, 1, H, W])
